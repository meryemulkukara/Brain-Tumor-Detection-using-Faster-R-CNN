{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvtSgS8VM-BW",
        "outputId": "c87b1617-5019-4536-86a0-82a02ba81009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tumor_dataset_path=\"/content/drive/MyDrive/Colab Notebooks/tumor\""
      ],
      "metadata": {
        "id": "78ZA4b_MNF3d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def read_dataset(dataset_path):\n",
        "    train_images = []\n",
        "    train_labels = []\n",
        "    test_images = []\n",
        "    test_labels = []\n",
        "\n",
        "    # Navigating the dataset directory\n",
        "    for class_folder in os.listdir(dataset_path):\n",
        "        class_folder_path = os.path.join(dataset_path, class_folder)\n",
        "        if os.path.isdir(class_folder_path):\n",
        "            train_images_folder_path = os.path.join(class_folder_path, 'images', 'train')\n",
        "            train_labels_folder_path = os.path.join(class_folder_path, 'labels', 'train')\n",
        "            test_images_folder_path = os.path.join(class_folder_path, 'images', 'test')\n",
        "            test_labels_folder_path = os.path.join(class_folder_path, 'labels', 'test')\n",
        "\n",
        "            # Training dataset: Navigating files in image folder\n",
        "            for image_file in os.listdir(train_images_folder_path):\n",
        "                if image_file.endswith('.jpg'):\n",
        "                    image_path = os.path.join(train_images_folder_path, image_file)\n",
        "                    label_file = os.path.join(train_labels_folder_path, image_file.replace('.jpg', '.txt'))\n",
        "\n",
        "                    # Checking the existence of the label file\n",
        "                    if not os.path.exists(label_file):\n",
        "                        continue\n",
        "\n",
        "                    # Reading the image\n",
        "                    image = cv2.imread(image_path)\n",
        "                    train_images.append(image)\n",
        "\n",
        "                    # Reading the tag file\n",
        "                    with open(label_file, 'r') as f:\n",
        "                        labels = f.readlines()\n",
        "                    train_labels.append(labels)\n",
        "\n",
        "            # Test dataset: Navigating files in image folder\n",
        "            for image_file in os.listdir(test_images_folder_path):\n",
        "                if image_file.endswith('.jpg'):\n",
        "                    image_path = os.path.join(test_images_folder_path, image_file)\n",
        "                    label_file = os.path.join(test_labels_folder_path, image_file.replace('.jpg', '.txt'))\n",
        "\n",
        "                    # Checking the existence of the label file\n",
        "                    if not os.path.exists(label_file):\n",
        "                        continue\n",
        "\n",
        "                    # Reading the image\n",
        "                    image = cv2.imread(image_path)\n",
        "                    test_images.append(image)\n",
        "\n",
        "                    # Reading the tag file\n",
        "                    with open(label_file, 'r') as f:\n",
        "                        labels = f.readlines()\n",
        "                    test_labels.append(labels)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = read_dataset(tumor_dataset_path)\n"
      ],
      "metadata": {
        "id": "TzaQn5f_NMMW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# New size\n",
        "new_size = (224, 224)\n",
        "\n",
        "\n",
        "# Train data resizing\n",
        "resized_train_images = []\n",
        "resized_train_labels = []\n",
        "\n",
        "for i in range(len(train_images)):\n",
        "    image_array = train_images[i]\n",
        "    image = Image.fromarray(image_array)\n",
        "    resized_image = image.resize(new_size)\n",
        "    resized_train_images.append(resized_image)\n",
        "    \n",
        "    labels = train_labels[i]\n",
        "    #resized_labels = []\n",
        "    \n",
        "    \n",
        "    label_values = labels[0].split(' ')\n",
        "    x = float(label_values[1])\n",
        "    #print(label)\n",
        "    y = float(label_values[2])\n",
        "    width = float(label_values[3])\n",
        "    height = float(label_values[4])\n",
        "        \n",
        "    new_x = x * (new_size[0] / image.width)\n",
        "    new_y = y * (new_size[1] / image.height)\n",
        "    new_width = width * (new_size[0] / image.width)\n",
        "    new_height = height * (new_size[1] / image.height)\n",
        "\n",
        "    resized_labels = [new_x, new_y, new_width, new_height]\n",
        "    resized_train_labels.append(resized_labels)\n",
        "print(len(resized_train_labels)) \n",
        "print(len(resized_train_images))   \n",
        "\n",
        "# Test data resizing\n",
        "resized_test_images = []\n",
        "resized_test_labels = []\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    image_array = test_images[i]\n",
        "    image = Image.fromarray(image_array)\n",
        "    resized_image = image.resize(new_size)\n",
        "    resized_test_images.append(resized_image)\n",
        "    \n",
        "    labels = test_labels[i]\n",
        "    \n",
        "    label_values = labels[0].split(' ')\n",
        "    #print(label_values[0])\n",
        "    x = float(label_values[1])\n",
        "    y = float(label_values[2])\n",
        "    width = float(label_values[3])\n",
        "    height = float(label_values[4])\n",
        "        \n",
        "    new_x = x * (new_size[0] / image.width)\n",
        "    new_y = y * (new_size[1] / image.height)\n",
        "    new_width = width * (new_size[0] / image.width)\n",
        "    new_height = height * (new_size[1] / image.height)\n",
        "        \n",
        "    resized_labels= [new_x, new_y, new_width, new_height]\n",
        "    resized_test_labels.append(resized_labels)\n",
        "    #print(resized_test_labels[i])\n",
        "\n",
        "print(len(resized_test_labels)) \n",
        "print(len(resized_test_images)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn-qVsKFNqVf",
        "outputId": "d73ad339-ce83-42e7-cdc5-822f6a677d12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "878\n",
            "878\n",
            "223\n",
            "223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Define the RPN class to integrate the RPN component\n",
        "class RPN(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super(RPN, self).__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self.conv = nn.Conv2d(in_channels, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Output channel for bounding box coordinate estimates\n",
        "        self.reg_layer = nn.Conv2d(256, 4 * num_anchors, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # Output channel for class possibilities\n",
        "        self.cls_layer = nn.Conv2d(256, 2 * num_anchors, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        # for Anker boxes\n",
        "        self.num_anchors = num_anchors\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convolution layers\n",
        "        x = F.relu(self.conv(x))\n",
        "        #x = F.relu(self.conv(x.unsqueeze(-1).unsqueeze(-1)))\n",
        "\n",
        "\n",
        "        # boundibg box estimates\n",
        "        reg = self.reg_layer(x)\n",
        "        reg = reg.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
        "\n",
        "        # class possibilities\n",
        "        cls = self.cls_layer(x)\n",
        "        cls = cls.permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 2)\n",
        "\n",
        "        return reg, cls"
      ],
      "metadata": {
        "id": "bFOvRsjjY3-j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Load or create a pre-trained model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Step 2: Add the classification part or RPN component using the properties of the model\n",
        "num_features = model.fc.in_features\n",
        "print(num_features)\n",
        "num_classes = 3  # class number\n",
        "model.fc = nn.Linear(num_features, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66L2USprQNCx",
        "outputId": "55f45506-d6d8-4292-813c-0f878fca73e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Use Softmax loss for classification part, L2 loss for RPN component\n",
        "softmax_loss = nn.CrossEntropyLoss()\n",
        "l2_loss = nn.MSELoss()"
      ],
      "metadata": {
        "id": "1UkXatNbQzEc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_loss(reg, cls, train_labels):\n",
        "    # Initialize loss variables\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    # Iterate over each tensor in the train_labels list\n",
        "    for labels in train_labels:\n",
        "        # Reshape the labels tensor\n",
        "        #labels = labels.view(-1, 5)\n",
        "\n",
        "        # Get the actual bounding box coordinates\n",
        "        box_coordinates = labels[:, :3]\n",
        "\n",
        "        # Calculate classification loss (softmax loss)\n",
        "        classification_loss = torch.nn.functional.cross_entropy(cls, labels[:, 3].long())\n",
        "\n",
        "        # Calculate the L2 loss for the RPN component\n",
        "        regression_loss = torch.nn.functional.mse_loss(reg, box_coordinates)\n",
        "\n",
        "        # Calculate total loss (sum of softmax loss and L2 loss)\n",
        "        total_loss += classification_loss + regression_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "SNARCXnLhUZI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Choose an optimization algorithm and use the back propagation algorithm to train the model\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "SYfLcIZXhuwg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RPN component\n",
        "in_channels = 3  # Channel count of feature maps\n",
        "num_anchors = 7 # anker box number\n",
        "rpn = RPN(in_channels, num_anchors)"
      ],
      "metadata": {
        "id": "eNHPE6lzh1UJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "class CustomDataset():\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "#have variables resized_train_images and resized_train_labels\n",
        "train_dataset = CustomDataset(resized_train_images, resized_train_labels, transform=transform)\n",
        "\n",
        "batch_size = 32  # Batch size\n",
        "shuffle = True  # Set to True, to shuffle the training data\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n"
      ],
      "metadata": {
        "id": "EAa67QgMjPxV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images,labels in train_loader:\n",
        "  print(images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lshgcnbimRch",
        "outputId": "874e03fa-f116-4d58-b31b-529bea5d1bbc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([14, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    i=0\n",
        "    for images, labels in train_loader:\n",
        "        \n",
        "        # Get feature maps using model\n",
        "        features = model(images)\n",
        "        print(features.shape)\n",
        "        features = features.unsqueeze(2).unsqueeze(3)\n",
        "        features = features.repeat(1, 1, 224, 224)\n",
        "\n",
        "\n",
        "    \n",
        "        reg, cls = rpn(features)\n",
        "        print(labels)\n",
        "        desired_batch_size = 32\n",
        "        labels = [label.repeat(desired_batch_size // len(labels)) for label in labels]\n",
        "        labels = torch.cat(labels)[:desired_batch_size]\n",
        "\n",
        "\n",
        "\n",
        "        print(len(resized_train_labels[i]))\n",
        "        i=i+1\n",
        "        # Loss calculation\n",
        "        #resized_train_labels_tensor = torch.tensor(resized_train_labels[i])\n",
        "        labels_tensor = torch.unsqueeze(torch.tensor(resized_train_labels[i]), 0)\n",
        "        print(labels_tensor.dim())\n",
        "        loss = compute_loss(reg, cls,labels_tensor)\n",
        "        \n",
        "        #Backpropagation and gradient update\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step() \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "_ZisA-dJjbTF",
        "outputId": "036d5819-d4d5-4a90-9bef-c36e2e1952e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3])\n",
            "[tensor([0.1574, 0.3962, 0.2075, 0.5167, 0.2218, 0.1841, 0.2829, 0.5760, 0.6871,\n",
            "        0.1594, 0.4637, 0.1551, 0.2172, 0.2252, 0.1728, 0.1566, 0.7166, 0.1512,\n",
            "        0.1571, 0.2737, 0.2185, 0.4601, 0.3296, 0.1428, 0.2236, 0.2542, 0.2675,\n",
            "        0.1499, 0.3816, 0.2267, 0.2560, 0.1923], dtype=torch.float64), tensor([0.1307, 0.4293, 0.1189, 0.5515, 0.1692, 0.1887, 0.1204, 0.5007, 0.4570,\n",
            "        0.1653, 0.5736, 0.1646, 0.1225, 0.1227, 0.2244, 0.1933, 0.4693, 0.1579,\n",
            "        0.0575, 0.5582, 0.2416, 0.5936, 0.5648, 0.1607, 0.2008, 0.2413, 0.1386,\n",
            "        0.1458, 0.4231, 0.1350, 0.2485, 0.2511], dtype=torch.float64), tensor([0.0693, 0.1187, 0.0493, 0.1588, 0.0339, 0.0529, 0.0575, 0.1095, 0.2079,\n",
            "        0.0365, 0.1187, 0.0513, 0.0308, 0.0282, 0.0827, 0.0678, 0.1826, 0.0508,\n",
            "        0.0421, 0.1119, 0.0816, 0.1771, 0.0986, 0.0503, 0.0488, 0.1273, 0.0298,\n",
            "        0.0370, 0.1661, 0.0529, 0.0888, 0.0960], dtype=torch.float64), tensor([0.0745, 0.0822, 0.0560, 0.0657, 0.0416, 0.0354, 0.0621, 0.0729, 0.2383,\n",
            "        0.0329, 0.0750, 0.0324, 0.0508, 0.0308, 0.0688, 0.0806, 0.1602, 0.0508,\n",
            "        0.0318, 0.1284, 0.0724, 0.1294, 0.0739, 0.0513, 0.0534, 0.0904, 0.0257,\n",
            "        0.0380, 0.1315, 0.0483, 0.0626, 0.0483], dtype=torch.float64)]\n",
            "4\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a541dc266b08>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlabels_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_train_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#Backpropagation and gradient update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-20cdd7a7a8ef>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(reg, cls, train_labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Get the actual bounding box coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mbox_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Calculate classification loss (softmax loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vzid4BrmkD2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}